# Regression and model validation


*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

```{r}
date()
set.seed(0)

```

```{r}

library(dplyr)
library(tidyverse)
library(GGally)
library(ggplot2)
library(boot)
```

<!-- 
The joined data set used in the analysis  exercise combines the two student alcohol consumption data sets. The following adjustments have been made:

The variables not used for joining the two data have been combined by averaging (including the grade variables)
'alc_use' is the average of 'Dalc' and 'Walc'
'high_use' is TRUE if 'alc_use' is higher than 2 and FALSE otherwise

Read the joined student alcohol consumption data into R either from your local folder (if you completed the Data wrangling part) or from this url (in case you got stuck with the Data wrangling part): 
https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/alc.csv
(In the above linked file, the column separator is a comma and the first row includes the column names). Print out the names of the variables in the data and describe the data set briefly, assuming the reader has no previous knowledge of it.
-->

## Load data

```{r}

alc <- read_csv('data/alc.csv')
colnames(alc)
glimpse(alc)

```

This is a dataset combined from two open dataset, which are both collected from two Portuguese schools by school reports and questionnaires. The meta data of of the original dataset can be found from [here](https://archive.ics.uci.edu/ml/datasets/Student+Performance). Two extra variables has been added to the dataset `alc`:
* `alc_use`: the average of `Dalc` and `Walc`
* `high_use`: TRUE if `alc_use` is higher than 2 and FALSE otherwise

Overall, the dataset `alc` contains 370 observations (rows) with 35 variables (columns), which have been printed out above. 



## 4 variables and hypotheses
<!-- 
The purpose of your analysis is to study the relationships between high/low alcohol consumption and some of the other variables in the data. To do this, choose 4 interesting variables in the data and for each of them, present your personal hypothesis about their relationships with alcohol consumption. (0-1 point)
-->
I would like to choose the following variables  to study their relationship with alcohol consumption:
* age: Adult students are legally allowed to buy and drink alcohol, while minors are not allowed.
* freetime: A student who have more time after school may consume more alcohol
* goout: A student who going out more often with friends are more likely to be in a party or a similar situation. This means such a student may consume more alcohol.
* famrel: Parents will usually guide their kids well and prevent them from bad lifestyle. But a student with a terrible family relationship may easily have some bad habits. One of the problems may be alcohol addiction.

## Expore the distributions of my chosen variables and their relationships with alcohol consumption
<!-- 
Numerically and graphically explore the distributions of your chosen variables and their relationships with alcohol consumption (use for example cross-tabulations, bar plots and box plots). Comment on your findings and compare the results of your exploration to your previously stated hypotheses. (0-5 points)
-->
```{r}
alc %>% group_by(age) %>% summarise(alcohol_consumption = mean(alc_use))
ggplot(alc, aes(y = alc_use, x = age, group = age)) + geom_boxplot()
```
The table above shows there are 7 groups from 15 years old to 22 years old in the dataset. From this table, we can see there is a positive correlation between age and average alcohol consumption, except age group 19 and 20. This is generally aligned with the box chart I made, which shows that the distribution of alcohol consumption goes up when students getting older. This means my hypothesis about age and alcohol consumption generally holds.

```{r}
alc %>% group_by(freetime) %>% summarise(alcohol_consumption = mean(alc_use))
ggplot(alc, aes(y = alc_use, x = freetime, group = freetime)) + geom_boxplot()
```

This table shows a positive correlation between `freetime` and average `alc_use` as well. The box chart shows that the distribution of `alc_use` are more spread out and reach to a higher value, if there is a higher `freetime`. 

```{r}
alc %>% group_by(goout) %>% summarise(alcohol_consumption = mean(alc_use))
ggplot(alc, aes(y = alc_use, x = goout, group = goout)) + geom_boxplot()
```

Similar as the previous two variables, the result shows there is a positive correlation between `goout` and average `alc_use`. This is also confirmed with the box plot. My hypothesis about students who goes out more with friends may consume more alcohol generally holds true.

```{r}
alc %>% group_by(famrel) %>% summarise(alcohol_consumption = mean(alc_use))
ggplot(alc, aes(y = alc_use, x = famrel, group = famrel)) + geom_boxplot()
```

This table shows a negative correlation between `famrel` and average `alc_use`, which suggests a worse family relationship can generally leads to more alcohol consumption. The box plot aligns with my hypothesis as well. Generally, the worse a `famrel` is, the higher value `alc_use` is distributed. Dispite the mean, upper quartile and upper whisker of the worst `famrel` box is lower than the second worst one, its lower  quatile is the highest, which may still suggest my hypothesis holds true.

## Logistic regression

<!-- 
Use logistic regression to statistically explore the relationship between your chosen variables and the binary high/low alcohol consumption variable as the target variable. Present and interpret a summary of the fitted model. Present and interpret the coefficients of the model as odds ratios and provide confidence intervals for them. Interpret the results and compare them to your previously stated hypothesis. Hint: If your model includes factor variables see for example the RHDS book or the first answer of this stackexchange thread on how R treats and how you should interpret these variables in the model output (or use some other resource to study this). (0-5 points)
-->

```{r}
model <- glm(high_use ~ age + freetime + goout + famrel, data = alc, family = "binomial")
summary(model)

```

As what the model summary shows, `goout` and `famrel` are the two most significant variables when predicting `high_use`. The `Pr(>|t|)` of these two variables are 6.38e-09 and 0.00199 respectively. The other two variables `age` and `freetime` are less significant.

```{r}
OR <- coef(model) %>% exp
CI <- confint(model)
cbind(OR, CI)
```

According to the odd ratio tables, we can see that `goout` has a strong positive connection with `high_use` (OR > 2), while `famrel` has a strong negative relationship with `high_use` (OR < 1). `age` and `freetime` also have a positive relationship with `high_use`. But they are less significant as `goout`. This suggests my previous hypothesis generally holds true

## Explore the predictive power of my model

<!-- 
Using the variables which, according to your logistic regression model, had a statistical relationship with high/low alcohol consumption, explore the predictive power of you model. Provide a 2x2 cross tabulation of predictions versus the actual values and optionally display a graphic visualizing both the actual values and the predictions. Compute the total proportion of inaccurately classified individuals (= the training error) and comment on all the results. Compare the performance of the model with performance achieved by some simple guessing strategy. (0-3 points)
-->
```{r}

alc <- mutate(alc, probability = predict(model, type = "response"))
alc <- mutate(alc, prediction = probability > 0.5)

g <- ggplot(alc, aes(x = high_use, y = probability, col = prediction))
g + geom_point()

table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table %>% addmargins


loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}



loss_func(class = alc$high_use, prob = alc$prediction)
loss_func(class = alc$high_use, prob = runif(nrow(alc), 0, 1) > 0.5) # Random guessing
```


The training error is 24.6%, which is lower than fliping a coin without any proof and guessing 1 for all observations. This means the variables I selected and the model I trained are effective. However, guessing 0 for all achieves 30% error rate, which is close to my result. This suggest the data itself is not balanced, but it also makes sense because high school students are generally less likely to have a drinking problem.

## Bonus: 10-fold cross-validation
<!-- 
Bonus: Perform 10-fold cross-validation on your model. Does your model have better test set performance (smaller prediction error using 10-fold cross-validation) compared to the model introduced in the Exercise Set (which had about 0.26 error). Could you find such a model? 
-->
```{r}

model_2 <- glm(high_use ~ absences + failures + goout + famrel, data = alc, family = "binomial")
cv <- cv.glm(data = alc, cost = loss_func, glmfit = model_2, K = 10)
cv$delta[1]
```

I replaced the two least significant variables in my initial model with the two most significant variables in the model introduced in the Exercise Set. The result is slightly better than the result in Exercise Set

## Super-Bonus: Compare the performance of different logistic regression models

<!-- 
Super-Bonus: Perform cross-validation to compare the performance of different logistic regression models (= different sets of predictors). Start with a very high number of predictors and explore the changes in the training and testing errors as you move to models with less predictors. Draw a graph displaying the trends of both training and testing errors by the number of predictors in the model.
-->


```{r}
all_predictors <- rev(c('goout', 'absences', 'failures', 'famrel', 'health', 'sex'))
num_predictors <- length(all_predictors):1

train_losses <- c()
test_losses <- c()

for( i in num_predictors){
    predictors <- all_predictors[1:i]
    f <- as.formula(paste('high_use', paste(predictors, collapse = ' + '), sep = ' ~ '))
    glm <- glm(f, data = alc, family = "binomial")
    alc <- mutate(alc, probability = predict(glm, type = "response"))
    alc <- mutate(alc, prediction = probability > 0.5)
    train_loss <- loss_func(class = alc$high_use, prob = alc$prediction)
    train_losses <- append(train_losses, train_loss)

    cv <- cv.glm(data = alc, cost = loss_func, glmfit = glm, K = 10)
    test_loss <- cv$delta[1]
    test_losses <- append(test_losses, test_loss)
    
}


df = data.frame(num_predictors, train_losses, test_losses)
ggplot(df, aes(num_predictors)) +                    # basic graphical object
  geom_line(aes(y=train_losses, colour="Train")) +  # first layer
  geom_line(aes(y=test_losses, colour="Test")) +  # second layer
  scale_color_manual(name = "Losses", values = c("Train" = "darkblue", "Test" = "red")) + 
  ylab("Loss")
```

