
# Clustering and classification

```{r}
date()
set.seed(2)
library(MASS)
library(dplyr)
library(tidyverse)
library(corrplot)
library(ggplot2)
library(plotly)
```


## Load data
<!--Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. Details about the Boston dataset can be seen for example here. (0-1 points)-->

```{r}
data("Boston")
str(Boston)
summary(Boston)
```

This is the Boston dataset coming from MASS package of R. The dataset records the statistics related to the housing values in suburbs of Boston. As what we can see from above, it contains 506 observations and 14 variables. All variables is recorded as a number. Most of variables are float numbers, except `chas` and `rad` are intergers. The description of each variables can be found from [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).

## Graphical overview and variable summary
<!--Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-2 points)-->

```{r}
cor_matrix <- cor(Boston) %>% round(digits=2)
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
```

The plot above is a visualization of the correlation matrix of variables in the Boston dataset. The plot summarize the interrelationship among all the variables of the dataset. As what we can see from the plot, most of variables are more or less correlated with each other, except `chas`. It seems `chas` is independant from the rest of variables. `indus` is probably the variable with the most correlation with other variables (except `chas`). It has a strong positive correlation with `nox` and a stong negative correlation with `dis`.

## Standardize the dataset and scaled data summary
<!--Standardize the dataset and print out summaries of the scaled data. How did the variables change? Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)-->


After standardizing, the variables will be scaled so that the mean of each variable is zero and the standard deviation is one. The summary of the scaled dataset is listed below:

```{r}
boston_scaled <- as.data.frame(scale(Boston))
summary(boston_scaled)
```

```{r}
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = quantile(boston_scaled$crim), include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

n <- nrow(boston_scaled) # number of rows in the Boston dataset
ind <- sample(n,  size = n * 0.8) # choose randomly 80% of the rows
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

## Fit an LDA and draw its biplot
<!--Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot. (0-3 points)-->
```{r}
lda.fit <- lda(crime ~ ., data = train) # linear discriminant analysis

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

The biplot above illustrates an LDA trained with categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. According to this plot, `rad` has the most variation. `zn` and `nox` have slightly more variation than other predictor variables.

## Validate LDA
<!--Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)-->
```{r}
correct_classes <- test$crime # Save the crime categories from the test set
test <- dplyr::select(test, -crime) # remove the categorical crime variable
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

It seems the LDA model runs quite well on the test set, where most of observations are classified into the correct categories. The error rate of this LDA is around 23% 

## K-means
<!--Reload the Boston dataset and standardize the dataset (we did not do this in the Exercise Set, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)-->

I calculate the distances between the observations and the summary is printed below:

```{r}

data("Boston")
boston_scaled <- as.data.frame(scale(Boston))
dist_eu <- dist(boston_scaled) # Euclidean distance of Boston
summary(dist_eu)
```

```{r}
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

The plot above suggests the optimal number of clusters is 2, as WCSS drops the most at 2.

```{r}
km <- kmeans(boston_scaled, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```

The plot above shows how K-means cluster the dataset. Clusters are colored into red and black. It seems K-means works the best on `tax` and `rad`, as there is an obvious separation between two cluster.


## Bonus: Visualize a biplot of LDA based on K-means clustering
<!--Bonus: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influential linear separators for the clusters? (0-2 points to compensate any loss of points from the above exercises)-->


```{r}
data("Boston")
boston_scaled <- as.data.frame(scale(Boston))
km <- kmeans(Boston, centers = 3)

lda.fit <- lda(km$cluster ~ ., data = boston_scaled)
classes <- as.numeric(km$cluster)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

The plot above is biplot of K-means clustering for Boston dataset in two dimensional space. The number of cluster is 3. It seems K-means cluster the dataset fairly well. According to the plot above, `tax` and `rad` have more variation than the rest variables. Also, these two variables are almost independant to each other. 

## Super-Bonus: Visualize and compare LDA and K-means in 3D
<!--Super-Bonus: Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.
Next, install and access the plotly package. Create a 3D plot (cool!) of the columns of the matrix product using the code below.
Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities? (0-3 points to compensate any loss of points from the above exercises)-->

```{r}
lda.fit <- lda(crime ~ ., data = train)
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)

```

```{r}
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=~train$crime)

```

I tried running with K-means twice with different number of clusters: 2 (the optimal number of clustering) and 4 (the number of categories for `crime`). 

```{r}

km = kmeans(model_predictors, centers = 2)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=~factor(km$cluster))
```

When the number of clusters is 2, the 3D plot above shows that the K-means does a fairly good job as what I expected. The main body of two cluster are obvious. Comparing to the plot which colors observations according to `crime`, it seems one cluster stands for `high`, while the other one include all the rest of categories. Despite some observations are clustered into the `high` cluster, they are located much closer to the other cluster in 3D space. It seems that those observations mostly belong to `med_high`. I think the clusering makes sense considering `med_high` observations are logically more close to `high` observations.

```{r}

km = kmeans(model_predictors, centers = 4)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=~factor(km$cluster))
```

When the number of clusters is 4, the plot above seems to be more similar to the one colored by `crime`. Yet, there are observations that are mis-classified into another cluster. The most obvious ones are mis-classified between `high` cluster and `med_high` cluster, which is plausible considering they are logically next to each other. Other mis-classified observations are mostly located at the border of each cluster. I suppose those observations are quite difficult for the algorithm.